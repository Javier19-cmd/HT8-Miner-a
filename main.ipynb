{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoja de Trabajo 8\n",
    "\n",
    "### Roberto Ríos, 20979\n",
    "### Javier Valle, 20159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use los mismos conjuntos de entrenamiento y prueba que utilizó en las hojas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conjuntos de entrenamiento y prueba de la Hoja 5.\n",
    "\n",
    "# Abriendo el archivo clasficado.\n",
    "datos = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "# Limpieza de datos.\n",
    "datos = datos.replace(to_replace='',value=0)\n",
    "datos = datos.fillna(0)\n",
    "cat = datos.select_dtypes(include=['object']).columns.to_list()\n",
    "num = datos.select_dtypes(include=['number']).columns.to_list()\n",
    "\n",
    "# Pasando todo a un Dataframe.\n",
    "x = pd.DataFrame()\n",
    "x[num] = datos[num]\n",
    "x[cat] = datos[cat].apply(lambda x: pd.factorize(x)[0])\n",
    "res = 'Clasificacion'\n",
    "X = x.drop(res, axis=1)\n",
    "y = datos[res]\n",
    "\n",
    "# Imprimiendo la clasificación.\n",
    "print(\"Clasificación: \", y)\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Crea un clasificador de árbol de decisión\n",
    "clf = tree.DecisionTreeClassifier( max_depth=4 )\n",
    "\n",
    "# Entrena el clasificador en el conjunto de entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# # Accurancy.\n",
    "print(\"Precisión:\", clf.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "tree.plot_tree(clf, fontsize=4, feature_names=y, filled=True)\n",
    "\n",
    "# Abriendo el archivo clasficado.\n",
    "datos = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "# Limpieza de datos.\n",
    "datos = datos.replace(to_replace='',value=0)\n",
    "datos = datos.fillna(0)\n",
    "cat = datos.select_dtypes(include=['object']).columns.to_list()\n",
    "num = datos.select_dtypes(include=['number']).columns.to_list()\n",
    "\n",
    "# Pasando todo a un Dataframe.\n",
    "x = pd.DataFrame()\n",
    "x[num] = datos[num]\n",
    "x[cat] = datos[cat].apply(lambda x: pd.factorize(x)[0])\n",
    "res = 'OverallCond'\n",
    "X = x.drop(res, axis=1)\n",
    "y = datos[res]\n",
    "\n",
    "# Imprimiendo la clasificación.\n",
    "print(\"OverallCond: \", y)\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Crea un clasificador de árbol de decisión\n",
    "clf = tree.DecisionTreeClassifier( max_depth=4 )\n",
    "\n",
    "# Entrena el clasificador en el conjunto de entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Accurancy.\n",
    "print(\"Precisión:\", clf.score(X_test, y_test))\n",
    "\n",
    "# Haciendo una predicción.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicción:\", y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "tree.plot_tree(clf, fontsize=4, feature_names=y, filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Seleccione como variable respuesta la que creó con las categorías del precio de la casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "1455    0\n",
       "1456    1\n",
       "1457    1\n",
       "1458    0\n",
       "1459    0\n",
       "Name: Clasificacion, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abriendo el archivo.\n",
    "datos = pd.read_csv(\"archivo_clasificado.csv\")\n",
    "\n",
    "# Quitando los na del dataset.\n",
    "datos = datos.replace(to_replace='',value=0)\n",
    "datos = datos.fillna(0)\n",
    "\n",
    "datos\n",
    "\n",
    "# Usando como variable de respuesta la variable categórica que especifica si la casa es barata, media o cara.\n",
    "res = \"Clasificacion\"\n",
    "\n",
    "datos[res]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Genere  dos modelos  de  redes  neuronales  que  sean  capaz  de  clasificar  usando  la variable respuesta que categoriza las casas en baratas, medias y caras. Estos modelos deben tener  diferentes topologías y funciones de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20038ba3130>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abriendo el archivo clasficado.\n",
    "df = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Clasificacion'] = le.fit_transform(df['Clasificacion'])\n",
    "\n",
    "X = df.drop('Clasificacion', axis=1)\n",
    "y = df['Clasificacion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir y_train a una variable categórica one-hot encoded\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Imputar valores nulos en X_train y X_test\n",
    "X_train = X_train.fillna('missing')\n",
    "X_test = X_test.fillna('missing')\n",
    "\n",
    "# One-hot encoding de variables categóricas en X_train y X_test\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# Modelo 1: red neuronal con una capa oculta de 8 neuronas y función de activación ReLU\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelo\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Modelo 2: red neuronal con dos capas ocultas de 16 y 8 neuronas y función de activación sigmoidal\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(8, activation='sigmoid'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelos\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelos\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "model2.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use los modelos para predecir el valor de la variable respuesta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "Predicción 1:  [[1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [2.716525e-26 1.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]]\n",
      "Predicción 2:  [[0.7064693  0.274833   0.01562306 0.00307464]\n",
      " [0.7064693  0.274833   0.01562306 0.00307464]\n",
      " [0.7064693  0.274833   0.01562306 0.00307464]\n",
      " ...\n",
      " [0.7064693  0.274833   0.01562306 0.00307464]\n",
      " [0.7064693  0.274833   0.01562306 0.00307464]\n",
      " [0.7064693  0.274833   0.01562306 0.00307464]]\n"
     ]
    }
   ],
   "source": [
    "# Abriendo el archivo clasficado.\n",
    "df = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Clasificacion'] = le.fit_transform(df['Clasificacion'])\n",
    "\n",
    "X = df.drop('Clasificacion', axis=1)\n",
    "y = df['Clasificacion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir y_train a una variable categórica one-hot encoded\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Imputar valores nulos en X_train y X_test\n",
    "X_train = X_train.fillna('missing')\n",
    "X_test = X_test.fillna('missing')\n",
    "\n",
    "# One-hot encoding de variables categóricas en X_train y X_test\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# Modelo 1: red neuronal con una capa oculta de 8 neuronas y función de activación ReLU\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelo\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Modelo 2: red neuronal con dos capas ocultas de 16 y 8 neuronas y función de activación sigmoidal\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(8, activation='sigmoid'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelos\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelos\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "model2.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Hacer predicciones con el modelo 1\n",
    "y_pred1 = model1.predict(X_train)\n",
    "\n",
    "# Hacer predicciones con el modelo 2\n",
    "y_pred2 = model2.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"Predicción 1: \", y_pred1)\n",
    "print(\"Predicción 2: \", y_pred2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Haga las matrices de confusión respectivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "Predicción 1:  [[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n",
      "Predicción 2:  [[0.43294168 0.3317805  0.14183202 0.09344583]\n",
      " [0.43294168 0.3317805  0.14183202 0.09344583]\n",
      " [       nan        nan        nan        nan]\n",
      " ...\n",
      " [0.43294168 0.3317805  0.14183202 0.09344583]\n",
      " [0.43294168 0.3317805  0.14183202 0.09344583]\n",
      " [0.43294168 0.3317805  0.14183202 0.09344583]]\n",
      "Matriz de confusión modelo 1:\n",
      "[[823   0   0   0]\n",
      " [324   0   0   0]\n",
      " [ 19   0   0   0]\n",
      " [  2   0   0   0]]\n",
      "Matriz de confusión modelo 2:\n",
      "[[823   0   0   0]\n",
      " [324   0   0   0]\n",
      " [ 19   0   0   0]\n",
      " [  2   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Abriendo el archivo clasificado.\n",
    "df = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Clasificacion'] = le.fit_transform(df['Clasificacion'])\n",
    "\n",
    "X = df.drop('Clasificacion', axis=1)\n",
    "y = df['Clasificacion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding de variables categóricas en X_train y X_test\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# Convertir y_train a una variable categórica one-hot encoded\n",
    "num_classes = len(np.unique(y_train))\n",
    "if num_classes > 2:\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Modelo 1: red neuronal con una capa oculta de 8 neuronas y función de activación ReLU\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelo\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Modelo 2: red neuronal con dos capas ocultas de 16 y 8 neuronas y función de activación sigmoidal\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(8, activation='sigmoid'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelos\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Hacer predicciones con el modelo 1\n",
    "y_pred1 = model1.predict(X_train)\n",
    "y_pred1_classes = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Hacer predicciones con el modelo 2\n",
    "y_pred2 = model2.predict(X_train)\n",
    "y_pred2_classes = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "print(\"Predicción 1: \", y_pred1)\n",
    "print(\"Predicción 2: \", y_pred2)\n",
    "\n",
    "# Crear la matriz de confusión para cada modelo\n",
    "if num_classes == 2:\n",
    "    cm1 = confusion_matrix(y_train, y_pred1_classes)\n",
    "    cm2 = confusion_matrix(y_train, y_pred2_classes)\n",
    "else:\n",
    "    cm1 = confusion_matrix(np.argmax(y_train, axis=1), y_pred1_classes)\n",
    "    cm2 = confusion_matrix(np.argmax(y_train, axis=1), y_pred2_classes)\n",
    "\n",
    "print(\"Matriz de confusión modelo 1:\")\n",
    "print(cm1)\n",
    "print(\"Matriz de confusión modelo 2:\")\n",
    "print(cm2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare los resultados obtenidos con los diferentes modelos de clasificación usando redes neuronales en cuanto a efectividad, tiempo de procesamiento y equivocaciones (donde el algoritmo  se  equivocó  más,  donde  se  equivocó  menos  y  la  importancia  que  tienen  los errores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "Predicción 1:  [[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n",
      "Predicción 2:  [[0.1363871  0.33541837 0.3926438  0.13555074]\n",
      " [0.1363871  0.33541837 0.3926438  0.13555074]\n",
      " [       nan        nan        nan        nan]\n",
      " ...\n",
      " [0.1411127  0.35438678 0.364741   0.13975957]\n",
      " [0.1363871  0.33541837 0.3926438  0.13555074]\n",
      " [0.1363871  0.33541837 0.3926438  0.13555074]]\n",
      "Matriz de confusión modelo 1:\n",
      "[[823   0   0   0]\n",
      " [324   0   0   0]\n",
      " [ 19   0   0   0]\n",
      " [  2   0   0   0]]\n",
      "Matriz de confusión modelo 2:\n",
      "[[213   0 610   0]\n",
      " [ 64   0 260   0]\n",
      " [  2   0  17   0]\n",
      " [  0   0   2   0]]\n",
      "Accuracy modelo 1:  0.7046232876712328\n",
      "Accuracy modelo 2:  0.1969178082191781\n"
     ]
    }
   ],
   "source": [
    "# Abriendo el archivo clasificado.\n",
    "df = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Clasificacion'] = le.fit_transform(df['Clasificacion'])\n",
    "\n",
    "X = df.drop('Clasificacion', axis=1)\n",
    "y = df['Clasificacion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding de variables categóricas en X_train y X_test\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# Convertir y_train a una variable categórica one-hot encoded\n",
    "num_classes = len(np.unique(y_train))\n",
    "if num_classes > 2:\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Modelo 1: red neuronal con una capa oculta de 8 neuronas y función de activación ReLU\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelo\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Modelo 2: red neuronal con dos capas ocultas de 16 y 8 neuronas y función de activación sigmoidal\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(8, activation='sigmoid'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelos\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Hacer predicciones con el modelo 1\n",
    "y_pred1 = model1.predict(X_train)\n",
    "y_pred1_classes = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Hacer predicciones con el modelo 2\n",
    "y_pred2 = model2.predict(X_train)\n",
    "y_pred2_classes = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "print(\"Predicción 1: \", y_pred1)\n",
    "print(\"Predicción 2: \", y_pred2)\n",
    "\n",
    "# Crear la matriz de confusión para cada modelo\n",
    "if num_classes == 2:\n",
    "    cm1 = confusion_matrix(y_train, y_pred1_classes)\n",
    "    cm2 = confusion_matrix(y_train, y_pred2_classes)\n",
    "else:\n",
    "    cm1 = confusion_matrix(np.argmax(y_train, axis=1), y_pred1_classes)\n",
    "    cm2 = confusion_matrix(np.argmax(y_train, axis=1), y_pred2_classes)\n",
    "\n",
    "print(\"Matriz de confusión modelo 1:\")\n",
    "print(cm1)\n",
    "print(\"Matriz de confusión modelo 2:\")\n",
    "print(cm2)\n",
    "\n",
    "if num_classes == 2:\n",
    "    acc1 = accuracy_score(y_train, y_pred1_classes)\n",
    "    acc2 = accuracy_score(y_train, y_pred2_classes)\n",
    "else:\n",
    "    acc1 = accuracy_score(np.argmax(y_train, axis=1), y_pred1_classes)\n",
    "    acc2 = accuracy_score(np.argmax(y_train, axis=1), y_pred2_classes)\n",
    "\n",
    "print(\"Accuracy modelo 1: \", acc1)\n",
    "print(\"Accuracy modelo 2: \", acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Analice si no hay sobreajuste en los modelos. Use para esto la curva de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 1s 4ms/step\n",
      "Predicción 1:  [[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " ...\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n",
      "Predicción 2:  [[0.22803628 0.5078855  0.19872458 0.06535369]\n",
      " [0.22803628 0.5078855  0.19872458 0.06535369]\n",
      " [       nan        nan        nan        nan]\n",
      " ...\n",
      " [0.2378279  0.5025964  0.19610815 0.06346759]\n",
      " [0.22803628 0.5078855  0.19872458 0.06535369]\n",
      " [0.22803628 0.5078855  0.19872458 0.06535369]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<keras.engine.sequential.Sequential object at 0x0000020045EE7A90>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[0;32m    864\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    865\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 76\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicción 2: \u001b[39m\u001b[39m\"\u001b[39m, y_pred2)\n\u001b[0;32m     53\u001b[0m \u001b[39m# # Crear la matriz de confusión para cada modelo\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m# if num_classes == 2:\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m#     cm1 = confusion_matrix(y_train, y_pred1_classes)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m# print(\"Accuracy modelo 1: \", acc1)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m# print(\"Accuracy modelo 2: \", acc2)\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m train_sizes, train_scores, test_scores \u001b[39m=\u001b[39m learning_curve(\n\u001b[0;32m     77\u001b[0m     model1, X_train, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     79\u001b[0m train_scores_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m train_scores_std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(train_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:1597\u001b[0m, in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1595\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m-> 1597\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   1598\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m   1599\u001b[0m         clone(estimator),\n\u001b[0;32m   1600\u001b[0m         X,\n\u001b[0;32m   1601\u001b[0m         y,\n\u001b[0;32m   1602\u001b[0m         scorer,\n\u001b[0;32m   1603\u001b[0m         train,\n\u001b[0;32m   1604\u001b[0m         test,\n\u001b[0;32m   1605\u001b[0m         verbose,\n\u001b[0;32m   1606\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1607\u001b[0m         fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m   1608\u001b[0m         return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1609\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m   1610\u001b[0m         return_times\u001b[39m=\u001b[39;49mreturn_times,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1612\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m train_test_proportions\n\u001b[0;32m   1613\u001b[0m )\n\u001b[0;32m   1614\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1615\u001b[0m train_scores \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mtrain_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_unique_ticks)\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    870\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    871\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 873\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    874\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m---> 59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:1599\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1595\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[0;32m   1597\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   1598\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m-> 1599\u001b[0m         clone(estimator),\n\u001b[0;32m   1600\u001b[0m         X,\n\u001b[0;32m   1601\u001b[0m         y,\n\u001b[0;32m   1602\u001b[0m         scorer,\n\u001b[0;32m   1603\u001b[0m         train,\n\u001b[0;32m   1604\u001b[0m         test,\n\u001b[0;32m   1605\u001b[0m         verbose,\n\u001b[0;32m   1606\u001b[0m         parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1607\u001b[0m         fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[0;32m   1608\u001b[0m         return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1609\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m   1610\u001b[0m         return_times\u001b[39m=\u001b[39mreturn_times,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1612\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m train_test_proportions\n\u001b[0;32m   1613\u001b[0m )\n\u001b[0;32m   1614\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1615\u001b[0m train_scores \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mtrain_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_unique_ticks)\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:79\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     74\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                 \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou should provide an instance of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscikit-learn estimator instead of a class.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     86\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m     87\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.sequential.Sequential object at 0x0000020045EE7A90>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "# Abriendo el archivo clasificado.\n",
    "df = pd.read_csv('archivo_clasificado.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Clasificacion'] = le.fit_transform(df['Clasificacion'])\n",
    "\n",
    "X = df.drop('Clasificacion', axis=1)\n",
    "y = df['Clasificacion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding de variables categóricas en X_train y X_test\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# Convertir y_train a una variable categórica one-hot encoded\n",
    "num_classes = len(np.unique(y_train))\n",
    "if num_classes > 2:\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Modelo 1: red neuronal con una capa oculta de 8 neuronas y función de activación ReLU\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar modelo\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Modelo 2: red neuronal con dos capas ocultas de 16 y 8 neuronas y función de activación sigmoidal\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model2.add(Dense(8, activation='sigmoid'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar modelos\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Hacer predicciones con el modelo 1\n",
    "y_pred1 = model1.predict(X_train)\n",
    "y_pred1_classes = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Hacer predicciones con el modelo 2\n",
    "y_pred2 = model2.predict(X_train)\n",
    "y_pred2_classes = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "print(\"Predicción 1: \", y_pred1)\n",
    "print(\"Predicción 2: \", y_pred2)\n",
    "\n",
    "# # Crear la matriz de confusión para cada modelo\n",
    "# if num_classes == 2:\n",
    "#     cm1 = confusion_matrix(y_train, y_pred1_classes)\n",
    "#     cm2 = confusion_matrix(y_train, y_pred2_classes)\n",
    "# else:\n",
    "#     cm1 = confusion_matrix(np.argmax(y_train, axis=1), y_pred1_classes)\n",
    "#     cm2 = confusion_matrix(np.argmax(y_train, axis=1), y_pred2_classes)\n",
    "\n",
    "# print(\"Matriz de confusión modelo 1:\")\n",
    "# print(cm1)\n",
    "# print(\"Matriz de confusión modelo 2:\")\n",
    "# print(cm2)\n",
    "\n",
    "# if num_classes == 2:\n",
    "#     acc1 = accuracy_score(y_train, y_pred1_classes)\n",
    "#     acc2 = accuracy_score(y_train, y_pred2_classes)\n",
    "# else:\n",
    "#     acc1 = accuracy_score(np.argmax(y_train, axis=1), y_pred1_classes)\n",
    "#     acc2 = accuracy_score(np.argmax(y_train, axis=1), y_pred2_classes)\n",
    "\n",
    "# print(\"Accuracy modelo 1: \", acc1)\n",
    "# print(\"Accuracy modelo 2: \", acc2)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model1, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Entrenamiento\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Validación cruzada\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Seleccione ahora el SalesPrice como variable respuesta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abriendo el archivo.\n",
    "datos = pd.read_csv(\"archivo_clasificado.csv\")\n",
    "\n",
    "# Quitando los na del dataset.\n",
    "datos = datos.replace(to_replace='',value=0)\n",
    "datos = datos.fillna(0)\n",
    "\n",
    "datos\n",
    "\n",
    "# Usando como variable de respuesta la variable categórica que especifica si la casa es barata, media o cara.\n",
    "res = \"SalePrice\"\n",
    "\n",
    "datos[res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
